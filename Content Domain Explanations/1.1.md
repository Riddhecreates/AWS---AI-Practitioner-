**Deﬁne basic AI terms (for example, AI, ML, deep learning, neural networks, computer vision,
natural language processing [NLP], model, algorithm, training and inferencing, bias, fairness, ﬁt,
large language models(LLMs))**

*Artificial Intelligence (AI)*

Artificial Intelligence (AI) is the broad field focused on building systems that can perform tasks that normally require human intelligence, such as understanding language, recognizing images, or making decisions. AI is the umbrella concept that includes machine learning, deep learning, computer vision, NLP, and generative AI. Not all AI systems learn from data—some use fixed rules—but modern AI systems mostly rely on ML-based models. AI systems use algorithms to process data, produce a model, and then perform inferencing. In short, AI defines the goal, while ML and deep learning define how the goal is achieved.

*Machine Learning (ML)*

Machine Learning is a subset of AI where systems learn patterns from data instead of being explicitly programmed. During training, ML algorithms analyze labeled or unlabeled data to create a model that captures relationships within the data. Once trained, the model performs inferencing by making predictions on new data. ML is the foundation for most modern AI applications, including recommendation systems, fraud detection, and spam filtering. Deep learning is a specialized form of ML that handles more complex data using neural networks.

*Deep Learning*

Deep learning is a subset of machine learning that uses multi-layered neural networks to learn complex patterns from large amounts of data. It is especially effective for unstructured data like images, audio, and text, which traditional ML struggles with. Deep learning models require more computational power and data than basic ML models, but they achieve higher accuracy in tasks like computer vision and NLP. Technologies such as large language models (LLMs) and image recognition systems are built using deep learning. In short, deep learning is what enables modern AI systems to understand content at a human-like level.

*Neural Networks*

Neural networks are the core algorithms used in deep learning, inspired by how the human brain processes information. They consist of layers of connected nodes that transform input data into meaningful outputs. Each layer learns increasingly complex features, which is why deep networks perform well on tasks like speech recognition and image classification. Neural networks are trained using large datasets, and the trained network becomes a model used for inferencing. Without neural networks, deep learning and advanced AI systems like LLMs would not exist.

*Computer Vision*

Computer vision is a branch of AI that enables machines to see and understand visual data, such as images and videos. It relies heavily on deep learning and neural networks to identify patterns like shapes, edges, and objects. During training, models learn from labeled image datasets, and during inferencing, they recognize objects in new images. Computer vision is widely used in facial recognition, medical imaging, and self-driving cars. It demonstrates how AI, ML, deep learning, models, and inferencing work together in real-world systems.

*Natural Language Processing (NLP)*

Natural Language Processing (NLP) allows AI systems to understand, interpret, and generate human language. NLP combines machine learning, deep learning, and neural networks to process text and speech data. Models are trained on massive text datasets and then used for inferencing tasks like translation, summarization, and chatbots. Large language models (LLMs) are advanced NLP systems built using deep learning. NLP clearly shows how data, algorithms, training, models, and inferencing connect to create intelligent language-based applications.

*Model*

A model is the output of the training process in machine learning or deep learning. It represents learned patterns from data and is used to make predictions during inferencing. Models depend on algorithms and training data quality, which directly affects bias and fairness. Once deployed, models can perform real-time or batch inferencing. In AWS terms, services train models first and then use them repeatedly to generate predictions.

*Algorithm*

An algorithm is the mathematical method or procedure used to learn from data during training. Algorithms define how a model updates itself when exposed to new data. Neural networks, decision trees, and regression methods are all algorithms. The algorithm itself does not make predictions; the trained model does. Understanding this distinction is important for exam questions that separate training logic (algorithm) from prediction logic (model).

*Training and Inferencing*

Training is the process where an algorithm learns patterns from historical data to create a model. Inferencing happens after training, when the model is used to make predictions on new data. Training is usually computationally expensive and happens less frequently, while inferencing happens often and must be fast. AI systems depend on high-quality training data to ensure accurate and fair inferencing. This distinction is critical in AWS architecture and exam questions.

*Bias*

Bias occurs when a model produces unfair outcomes due to biased training data or design choices. Since models learn from historical data, any imbalance in that data becomes embedded in the model. Bias affects inferencing results and can lead to unfair decisions in hiring, lending, or healthcare systems. Bias is not caused by AI itself but by how training data is collected and used. Managing bias is a key responsibility in responsible AI.

*Fairness*

Fairness ensures AI models treat individuals and groups equitably across different demographics. It is closely related to bias and depends on careful data selection, model evaluation, and monitoring. Fairness does not mean identical outcomes, but rather unbiased and justified decision-making. AWS emphasizes fairness as part of Responsible AI practices. Fair models improve trust, compliance, and real-world usability.

*Fit*

Fit describes how well an AI or machine learning model learns the patterns in training data and how accurately it performs on new, unseen data. A well-fit model captures the true underlying relationship in the data without memorizing noise or missing important patterns. Fit is closely related to training, inferencing, and model performance, because poor fit leads to inaccurate or unreliable predictions. There are three common fit states: underfitting, good fit, and overfitting. Evaluating fit helps decide whether a model can be trusted in real-world AI systems.

*Large Language Models (LLMs)*

Large Language Models (LLMs) are a type of deep learning model trained on massive amounts of text data to understand and generate human language. They are built using neural networks and fall under Generative AI because they create new content rather than just analyzing existing data. LLMs use training to learn language patterns and inferencing to generate responses, summaries, or translations. Bias and fairness are important considerations because LLMs learn from large real-world datasets. LLMs combine AI, ML, deep learning, NLP, models, training, and inferencing into one system.For example: Text summarization, Question answering systems and Chat-based assistants.

**Describe the similarities and differences between AI, ML, Generative AI, and Deep Learning**

Artificial Intelligence (AI) is the broadest concept, focused on making machines perform tasks that normally require human intelligence. Machine Learning (ML) is a subset of AI where systems learn from data instead of following fixed rules. Deep Learning is a subset of ML that uses multi-layered neural networks to learn complex patterns, especially from unstructured data like images and text. Generative AI is built mainly on deep learning and focuses on creating new content such as text, images, or audio rather than just predicting outcomes. In short: AI defines the goal, ML defines learning from data, deep learning defines how complex learning happens, and Generative AI defines content creation.

Example:
A rule-based chatbot is AI but not ML.
A spam filter is ML.
Face recognition uses deep learning.
ChatGPT-style systems are Generative AI.

**Describe various types of inferencing (for example, batch and real-time)**

Inferencing is the process of using a trained model to make predictions on new data. Real-time inferencing happens instantly when data arrives and is used in applications like fraud detection or voice assistants, where immediate responses are required. Batch inferencing processes large volumes of data at scheduled intervals and is used when speed is less critical, such as monthly sales forecasting or customer segmentation. Both types use the same trained model but differ in timing and use cases. Choosing the correct inferencing type depends on business needs, cost, and latency requirements.

Example:
Credit card fraud detection → real-time inferencing
Monthly demand forecasting → batch inferencing

**Describe the different types of data in AI models**

AI models can use labeled or unlabeled data, depending on the learning approach. Labeled data includes known outputs and is used in supervised learning, while unlabeled data is used in unsupervised learning. Data can also be structured (tables, databases) or unstructured (images, text, audio). Common AI data types include tabular data for predictions, time-series data for trends, image data for computer vision, and text data for NLP. Deep learning is especially effective for handling unstructured data at scale.

Example:
Sales records → structured, tabular data
Customer reviews → unstructured, text data
Security camera footage → unstructured, image/video data

**Describe supervised learning, unsupervised learning, and reinforcement learning**

Supervised learning uses labeled data to train models for prediction tasks such as classification and regression. Unsupervised learning uses unlabeled data to discover hidden patterns, clusters, or relationships without predefined outputs. Reinforcement learning involves an agent that learns by interacting with an environment and receiving rewards or penalties based on its actions. These learning types differ in how data is used, but all rely on training models that perform inferencing later. AWS exam questions often test which learning type fits a given business problem.

Example:
Email spam detection → supervised learning
Customer segmentation → unsupervised learning
Game-playing AI or robotics → reinforcement learning

