**Describe the advantages of GenAI (for example, adaptability, responsiveness, simplicity).**

Generative AI refers to foundation models that can create new content (text, images, code) based on learned patterns from large datasets. In the AWS context, GenAI is typically accessed through managed services, focusing on scalability, flexibility, and reduced operational overhead rather than model building. If the question emphasizes business efficiency, speed, or ease of use, the answer usually points to GenAI advantages, not model architecture or training details.

*Adaptability*

A single foundation model can perform multiple tasks (summarization, Q&A, translation) using prompts instead of retraining. This works because the model learns generalized language patterns during pre-training.
Real-life example: An enterprise chatbot uses the same model to answer FAQs, summarize documents, and draft emails.
Exam cues: Look for phrases like “single model, multiple tasks” or “no retraining required”.

*Responsiveness*

GenAI systems can generate outputs in near real time once deployed, enabling interactive use cases. Managed inference endpoints handle scaling automatically.
Real-life example: Customer support assistants providing instant responses during live chats.
Exam cues: Keywords include “real-time,” “interactive,” and “low latency responses.”

*Simplicity*

Managed GenAI services abstract infrastructure, scaling, and model management. Users interact through APIs instead of building ML pipelines.
Real-life example: Teams using Amazon Bedrock to access multiple models without managing GPUs or training workflows.
Exam cues: Watch for “managed service,” “no ML expertise,” or “reduced operational complexity.”

**Identify disadvantages of GenAI solutions (for example, hallucinations, interpretability,  inaccuracy, nondeterminism).**

Generative AI systems produce outputs based on probability, not verified facts or rules. Because of this, they introduce reliability, control, and trust challenges that are important in enterprise and AWS-managed deployments. If a question highlights trust, explainability, or consistency concerns, the correct answer usually points to GenAI disadvantages rather than performance or scalability.

*Hallucinations*

The model may generate confident but false information because it predicts likely text, not factual correctness. This happens when training data lacks the answer or the prompt is ambiguous.
Real-life example: A chatbot invents product features that do not exist.
Exam cues: Keywords- “fabricated answers,” “confidently wrong,” “ungrounded output.”

*Inaccuracy*

Outputs can be partially correct but outdated or incomplete, especially when the model lacks real-time data access.
Real-life example: A GenAI assistant gives old pricing or deprecated policy information.
Exam cues: Look for “outdated,” “approximate,” or “not guaranteed correct.”

*Lack of Interpretability*

Foundation models operate as black boxes, making it difficult to explain why a specific output was generated.
Real-life example: An organization cannot justify why a specific recommendation was produced.
Exam cues: Phrases like “black box,” “cannot explain decisions,” or “limited transparency.”

*Nondeterminism*

The same prompt can produce different outputs due to randomness in token selection.
Real-life example: Repeated queries return slightly different summaries each time.
Exam cues: Watch for “non-repeatable,” “variable outputs,” or “probabilistic behavior.”

**Identify factors to consider when selecting GenAI models (for example, model types, performance requirements, capabilities, constraints, compliance).**

Selecting a GenAI model involves balancing capability, cost, risk, and governance. In AWS exam scenarios, the focus is on choosing models that meet business and compliance needs rather than technical optimization. If the question asks “which model should be chosen,” match the use case requirements first, then eliminate options that violate cost or compliance constraints.

*Model type*

Different models specialize in text, images, code, or multimodal tasks. Choosing the wrong type limits usefulness regardless of performance.
Real-life example: Using a text-only model for document summarization instead of an image model.
Exam cues: Keywords- “text vs multimodal,” “foundation model type,” “task-specific.”

*Performance requirements*

Latency, throughput, and accuracy requirements determine whether a lightweight or large model is appropriate. Higher performance usually increases cost.
Real-life example: Real-time chat assistants require low-latency models.
Exam cues: Look for “real-time,” “high throughput,” “low latency.”

*Capabilities*

Models vary in reasoning depth, context length, and language support. Capabilities must match the use case.
Real-life example: Long-document summarization requires models with large context windows.
Exam cues: Phrases like “long context,” “reasoning,” or “multilingual support.”

*Constraints (cost and scalability)*

Budget, usage limits, and scaling behavior affect sustainability. Managed services help control these constraints.
Real-life example: High-volume applications may require smaller, cheaper models.
Exam cues: Keywords- “cost control,” “scalability,” “pay-per-use.”

*Compliance and governance*

Data residency, privacy, and regulatory requirements influence model choice and deployment method.
Real-life example: Organizations avoiding models that retain customer data for training.
Exam cues: Look for “data privacy,” “regulatory compliance,” “governance.”

**Determine business value and metrics for GenAI applications (for example, cross-domain  performance, eﬃciency, conversion rate, average revenue per user, accuracy, customer lifetime value).**

The business value of GenAI is measured by how effectively it improves outcomes across functions, reduces cost, or increases revenue. In AWS exam questions, metrics are used to justify adoption rather than to evaluate model internals. If the question mentions business justification, choose metrics tied to revenue, cost reduction, or retention, not technical ML scores alone.

*Cross-domain performance*

A single GenAI system supports multiple business functions, increasing reuse and consistency. Value comes from avoiding separate tools.
Real-life example: One AI assistant supports sales, support, and internal documentation.
Exam cues: Keywords- “reuse,” “multiple departments,” “shared model.”

*Efficiency*

GenAI reduces manual work and time per task, improving operational efficiency.
Real-life example: Automated drafting of reports cuts preparation time.
Exam cues: Look for “time savings,” “automation,” “productivity gains.”

*Conversion rate*

Personalized or faster responses improve the percentage of users who complete desired actions.
Real-life example: AI-powered chat increases completed purchases on a website.
Exam cues: Phrases like “improved engagement,” “higher completion.”

*Average revenue per user (ARPU)*

GenAI-driven recommendations or upselling can increase revenue per customer.
Real-life example: AI suggesting add-ons during checkout.
Exam cues: Keywords- “upsell,” “monetization.”

*Accuracy*

Higher accuracy improves trust and reduces rework or escalation.
Real-life example: Fewer incorrect responses in customer support chats.
Exam cues: Look for “quality,” “error reduction.”

*Customer lifetime value (CLV)*

Better experiences and faster service increase retention over time.
Real-life example: AI support improves satisfaction, leading to repeat customers.
Exam cues: Phrases like “retention,” “long-term value.”



