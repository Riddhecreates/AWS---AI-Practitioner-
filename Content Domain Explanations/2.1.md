**Deﬁne foundational GenAI concepts (for example, tokens, chunking, embeddings, vectors, prompt engineering, transformer-based LLMs, foundation models [FMs], multimodal models, diﬀusion models)**

Generative AI (GenAI) focuses on models that can create new content such as text, images, audio, or code instead of just predicting labels. These systems are built using large neural networks trained on massive datasets. Concepts like tokens, embeddings, transformers, and foundation models explain how GenAI understands input, represents meaning, and generates output. On the exam, AWS tests whether you understand what each concept does and how they work together.

*Tokens*

Tokens are the basic units of text that a GenAI model processes. A token can be a word, part of a word, punctuation, or even a symbol. Models do not understand full sentences directly; they work by predicting the next token based on previous tokens. Tokenization affects cost, performance, and context length. More tokens mean higher compute cost and longer processing time. More tokens = higher cost and more compute

Example scenario: The sentence “Machine learning is powerful” may be split into tokens like: “Machine”, “ learning”, “ is”, “ powerful”.

Exam cue words: “Smallest text unit”, “token count”, “context length”

*Chunking*

Chunking is the process of splitting large text into smaller pieces so a model can process it efficiently. Since LLMs have context limits, chunking ensures important information fits within those limits. It is commonly used in retrieval-augmented generation (RAG). Proper chunking improves response accuracy and relevance. Poor chunking can cause loss of context. Use chunking when data is too large for a single prompt.

Example scenario: Breaking a long PDF into paragraphs before sending it to an LLM for question answering.

Exam cue words: “Context window”, “large documents”, “RAG”

*Embeddings*

Embeddings are numerical representations of meaning created from text, images, or other data. They allow models to understand semantic similarity rather than exact matches. Similar meanings produce embeddings that are close together in space. Embeddings are essential for search, recommendations, and retrieval. They act as the bridge between raw data and vectors. If meaning similarity is needed, think embeddings.

Example scenario: Searching documents based on meaning rather than keyword matching.

Exam cue words: “Semantic meaning”, “numerical representation”, “similarity search”

*Vectors*

Vectors are arrays of numbers that store embeddings in a mathematical space. They enable distance calculations such as cosine similarity. Vectors allow systems to quickly find related content. Vector databases store and search these efficiently. Every embedding is stored as a vector. Embeddings become vectors for search and retrieval.

Example scenario: Finding the most relevant document by comparing vector distances.

Exam cue words: “Vector space”, “distance”, “similarity”

*Prompt Engineering*

Prompt engineering is the practice of designing effective inputs to guide model output. It does not change model weights but influences responses. Techniques include instructions, examples, and role-based prompts. Good prompts improve accuracy and consistency. It is a low-cost way to improve results without retraining. If no training is involved, it’s prompt engineering.

Example scenario: Adding “Answer step by step” to get structured responses.

Exam cue words: “Input design”, “no retraining”, “instruction-based”

*Transformer-based LLMs*

Transformer-based LLMs use the transformer architecture to process sequences efficiently. They rely on attention mechanisms to understand relationships between tokens. This allows models to scale to billions of parameters. Transformers are the foundation of modern LLMs like GPT. They replaced older sequence models due to better performance.Modern LLMs = transformers + attention

Example scenario: Chatbots that maintain context across long conversations.

Exam cue words: “Attention mechanism”, “scalability”, “sequence modeling”

*Foundation Models (FMs)*

Foundation models are large, pre-trained models trained on diverse datasets. They can be adapted to many tasks using fine-tuning or prompts. FMs reduce the need for task-specific training. AWS emphasizes FMs in services like Amazon Bedrock. They form the base for GenAI applications. One model, many tasks = foundation model

Example scenario: Using a single LLM for summarization, translation, and chat.

Exam cue words: “Pre-trained”, “general-purpose”, “adaptable”

*Multimodal Models*

Multimodal models process multiple data types such as text, images, audio, and video. They understand relationships across modalities. These models can generate text from images or describe videos. They enable richer AI experiences. Multimodality expands GenAI beyond text. More than one data type = multimodal

Example scenario: Describing an image using natural language.

Exam cue words: “Text + image”, “multiple data types”

*Diffusion Models*

Diffusion models generate content by gradually removing noise from random data. They are commonly used for image and video generation. The process starts with noise and refines it step by step. These models produce high-quality visuals. They are different from text-based transformers. Image generation often = diffusion models

Example scenario: Generating realistic images from text prompts.

Exam cue words: “Noise removal”, “image generation”, “step-by-step refinement”

*Key points*

Tokens → embeddings → vectors = how meaning is represented

Chunking = fit data into context

Prompt engineering = control output without training

Transformers = architecture behind LLMs

Foundation models = reusable GenAI base

Multimodal + diffusion = expand beyond text

**Identify potential use cases for GenAI models (for example, image, video, and audio generation;  summarization; AI assistants; translation; code generation; customer service agents; search; recommendation engines)**

This question tests whether you understand where Generative AI adds value beyond traditional ML. GenAI is best when the output is new content, language-based, or creative, rather than a fixed prediction. Most GenAI use cases involve understanding, generating, or transforming unstructured data like text, images, audio, or code.

*Image, Video, and Audio Generation*

GenAI models can create new visual or audio content from prompts or examples. This is useful when human-created content is slow, expensive, or needs personalization at scale.
Real-life example: A marketing team generates product images or short ad videos automatically instead of hiring designers for every variation.
Exam cue words: content creation, synthetic data, generative output

*Text Summarization*

GenAI can condense long documents into short, meaningful summaries while preserving key points. This saves time and improves decision-making for humans.
Real-life example: Executives receive one-page summaries of long reports or customer feedback instead of reading hundreds of pages.
Exam cue words: summarization, knowledge extraction, productivity

*AI Assistants and Chatbots*

GenAI powers conversational systems that understand natural language and generate human-like responses. These systems assist users rather than replacing them entirely.
Real-life example: A customer support chatbot answers FAQs, drafts responses, and escalates complex cases to humans.
Exam cue words: conversational AI, human-in-the-loop, natural language

*Translation and Language Transformation*

GenAI models can translate text between languages and adjust tone or style. Unlike rule-based systems, they handle context and nuance better.
Real-life example: A global company automatically translates support tickets and emails for international teams.
Exam cue words: language translation, multilingual models, text transformation

*Code Generation and Developer Support*

GenAI can generate, explain, and refactor code, helping developers work faster but not replacing them.
Real-life example: A developer asks an AI assistant to generate boilerplate code or explain a complex function.
Exam cue words: developer productivity, code assistance, automation

*Customer Service Agents*

GenAI enables context-aware customer interactions, handling varied and unpredictable queries better than traditional chatbots.
Real-life example: An AI agent responds to customer complaints with personalized, empathetic messages based on past interactions.
Exam cue words: personalization, conversational systems, scalability

*Search and Information Retrieval*

GenAI improves search by understanding intent and generating direct answers, not just returning links.
Real-life example: An internal company search tool answers questions using documents instead of listing files.
Exam cue words: semantic search, intent understanding, embeddings

*Recommendation Engines*

GenAI enhances recommendations by explaining suggestions or generating personalized content, not just ranking items.
Real-life example: A streaming platform generates personalized movie descriptions based on user preferences.
Exam cue words: personalization, content generation, user engagement

Generative AI is most valuable when the task requires creating or transforming unstructured content, such as text, images, audio, or code. Unlike traditional ML, GenAI does not just predict outcomes—it produces new outputs based on learned patterns. It works best when assisting humans, scaling creativity, or handling language-heavy tasks. In exams, GenAI use cases usually involve generation, summarization, conversation, or personalization, not fixed rule-based decisions. If the question involves creating new content, understanding language, or interacting conversationally, GenAI is likely the correct answer. If the task is only predicting a number or category, traditional ML is usually more appropriate.

**Describe the foundation model lifecycle (for example, data selection, model selection, pre-
training, ﬁne-tuning, evaluation, deployment, feedback)**

This question checks whether you understand how foundation models (FMs) are built, adapted, and improved over time. Unlike traditional ML models trained once for a single task, foundation models go through a continuous lifecycle where large-scale training is followed by task-specific adaptation and ongoing feedback.

*Data Selection*

Foundation models start with massive, diverse datasets, usually unstructured data like text, images, audio, or video. The goal is broad knowledge, not task-specific accuracy.
Real-life example: An LLM is trained on books, articles, websites, and code repositories to learn general language patterns.
Exam cue words: large-scale data, diversity, unstructured data

*Model Selection*

At this stage, developers choose the model architecture and size, such as transformer-based models. The choice balances capability, cost, and deployment constraints.
Real-life example: A company selects a smaller foundation model for lower latency instead of a very large one.
Exam cue words: architecture, scalability, compute cost

*Pre-training*

Pre-training is where the foundation model learns general patterns from huge datasets, usually in a self-supervised way. This step is computationally expensive and done rarely.
Real-life example: Training a language model to predict the next word across billions of sentences.
Exam cue words: self-supervised learning, general knowledge, high compute

*Fine-tuning*

Fine-tuning adapts the pre-trained model to specific tasks or domains using smaller, labeled datasets. This makes the model practical for real-world use.
Real-life example: Fine-tuning a general LLM on customer support chats to improve response quality.
Exam cue words: task adaptation, domain-specific, smaller datasets

*Evaluation*

Evaluation checks whether the model meets technical and business requirements, such as accuracy, bias, latency, and safety. Multiple metrics are often used.
Real-life example: Testing whether an AI assistant gives correct and non-harmful answers.
Exam cue words: performance metrics, bias, validation

*Deployment*

Deployment makes the model available to users, often through APIs or managed services. This step considers scalability, cost, and reliability.
Real-life example: Deploying a fine-tuned model as an API for a mobile app.
Exam cue words: inference, production, scalability

*Feedback and Continuous Improvement*

User interactions and real-world data are collected as feedback to improve future versions of the model. This may lead to re-training or further fine-tuning.
Real-life example: Updating a chatbot after users report incorrect or biased responses.
Exam cue words: human feedback, iteration, lifecycle loop

The foundation model lifecycle is iterative and continuous, not one-time. Large, diverse data is used to pre-train a general model, which is then fine-tuned for specific tasks. Evaluation and deployment ensure the model is safe and useful in real environments. Feedback from real users closes the loop, helping improve accuracy, fairness, and relevance over time. If the question mentions huge datasets and general-purpose learning, think pre-training. If it mentions adapting to a specific task or domain, think fine-tuning. Feedback always implies continuous improvement, not a final step.
