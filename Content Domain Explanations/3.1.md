**Identify selection criteria to choose pre-trained models (for example, cost, modality, latency,  multi-lingual, model size, model complexity, customization, input/output length, prompt caching).**

Choosing a pre-trained GenAI model requires balancing performance, cost, and risk against the business use case. In the AWS AI Practitioner exam, questions test whether you can match model characteristics to requirements, not whether you can optimize models technically. In the exam, always eliminate models that exceed requirements. The best answer is the lowest-cost model that still meets modality, latency, and context needs.

*Cost*

Cost depends on factors such as token-based pricing, model size, and whether provisioned throughput is used. Larger and more capable models generally cost more per request. Organizations must consider both short-term usage cost and long-term scalability. Choosing the most powerful model is rarely cost-efficient if the task is simple.
Real-life example: Using a smaller text model for FAQ responses instead of a large reasoning-heavy model.
Exam cues: Keywords- “pay-per-token,” “cost control,” “budget constraints.”

*Modality*

Modality refers to the type of input and output a model supports, such as text-only or multimodal (text, image). Selecting the wrong modality limits functionality regardless of model quality. Multimodal models are more expensive and should only be chosen when required.
Real-life example: Choosing a text-only model for document summarization instead of an image-capable model.
Exam cues: Look for “text vs image,” “multimodal requirement.”

*Latency*

Latency measures how quickly a model returns responses. Low-latency models are critical for real-time or interactive applications but often cost more. High-latency models may be acceptable for batch or offline processing.
Real-life example: Live chat systems requiring near real-time responses.
Exam cues: Keywords- “real-time,” “interactive,” “response time.”

*Multilingual support*

Some models support multiple languages with high accuracy, while others are optimized for a single language. Multilingual models are necessary for global use cases but may increase cost or complexity. Language support must align with target users.
Real-life example: A customer support assistant serving users in multiple countries.
Exam cues: Phrases like “global users,” “multiple languages.”

*Model size*

Model size affects reasoning ability, accuracy, and resource usage. Larger models handle complex tasks better but consume more compute and cost more. Smaller models are faster and cheaper but may lack depth.
Real-life example: Using a small model for simple classification tasks.
Exam cues: “Large model” usually implies higher cost.

*Model complexity*

More complex models support advanced reasoning and longer contexts but are harder to control and explain. Simpler models are easier to manage and more predictable. Complexity should match the task, not exceed it.
Real-life example: Avoiding advanced reasoning models for basic text generation.
Exam cues: Keywords- “reasoning depth,” “control.”

*Customization*

Customization includes fine-tuning or prompt engineering to better align outputs with business needs. While customization improves relevance, it increases cost and governance effort. Managed customization is preferred over full retraining.
Real-life example: Prompt-tuning a model to follow a specific tone.
Exam cues: Look for “fine-tuning,” “domain adaptation.”

*Input and output length*

Models have limits on how much text they can process or generate. Long input or output requires models with larger context windows, which increases cost. Short tasks can use smaller-context models.
Real-life example: Summarizing long policy documents.
Exam cues: Keywords- “context window,” “long documents.”

*Prompt caching*

Prompt caching reuses previous prompts and responses to reduce repeated computation. This improves efficiency and lowers cost for repetitive workloads. Not all models or services support caching.
Real-life example: Reusing standard system prompts across thousands of requests.
Exam cues: Phrases like “repeated prompts,” “cost optimization.”

**Describe the eﬀect of inference parameters on model responses (for example, temperature,  input/output length).**

Inference parameters control how a GenAI model generates responses at runtime, without changing the model itself. In the AWS AI Practitioner exam, these parameters are important because they directly affect response quality, consistency, cost, and reliability, especially in production use cases. For exam questions, low temperature + controlled output length usually indicates enterprise, factual, or regulated use cases.

*Temperature*

Temperature controls randomness in model outputs. A low temperature makes the model more deterministic and predictable, producing similar answers for the same prompt. A high temperature increases randomness, leading to more diverse but less reliable outputs. Choosing the wrong temperature can either reduce creativity or increase hallucinations.
Real-life example: Using low temperature for factual Q&A and higher temperature for creative writing.
Exam cues: Keywords- “creativity vs consistency,” “deterministic output.”

*Input length*

Input length refers to how much text is provided to the model in a single request. Longer inputs give more context, which can improve relevance and accuracy. However, long inputs increase token usage, cost, and processing time, and may include unnecessary information.
Real-life example: Providing an entire document versus only the relevant sections for summarization.
Exam cues: Look for “context window,” “token usage,” “cost impact.”

*Output length*

Output length defines how much text the model is allowed to generate. Short outputs reduce cost and latency but may omit important details. Long outputs improve completeness but increase cost and the risk of irrelevant or incorrect information.
Real-life example: Limiting response length in customer support chatbots to control cost.
Exam cues: Keywords- “response limit,” “verbosity,” “token-based pricing.”

*Interaction between parameters*

Inference parameters do not operate independently. High temperature combined with long output length increases variability, cost, and risk of hallucinations. Careful tuning is required to balance reliability, cost, and usefulness.
Real-life example: Using low temperature and moderate output length for policy explanations.
Exam cues: Phrases like “parameter tuning,” “trade-offs.”

**Deﬁne Retrieval Augmented Generation (RAG) and describe its business applications (for example, Amazon Bedrock Knowledge Bases).**

Retrieval Augmented Generation (RAG) is a GenAI approach that combines a foundation model with an external knowledge source. Instead of relying only on what the model learned during training, RAG retrieves relevant documents at inference time and uses them as context. In AWS exam terms, RAG improves accuracy, trust, and relevance without retraining models. RAG works in two main steps: retrieval and generation. First, the system searches a knowledge base (documents, PDFs, databases) to find relevant information. Then, the retrieved content is injected into the model prompt so the model generates answers grounded in that data. This reduces hallucinations and keeps responses up to date.

RAG allows organizations to use their own data while keeping foundation models unchanged. This lowers cost and risk compared to fine-tuning. It also supports governance because source data can be controlled, updated, and audited independently. Businesses gain more reliable AI outputs aligned with internal knowledge. If the question says “use enterprise data,” “reduce hallucinations,” or “avoid retraining,” the correct answer is almost always RAG.

*Business applications of RAG*

1. Enterprise knowledge assistants
RAG enables AI assistants to answer questions using internal documents rather than public training data. This improves accuracy and trust for employees. Example: Internal policy or procedure Q&A systems.

2. Customer support and self-service
RAG grounds responses in approved documentation, reducing incorrect or fabricated answers. It improves first-contact resolution rates. Example: Support bots answering product or policy questions from official manuals.

3. Regulatory and compliance use cases
RAG ensures responses are based on controlled, compliant sources. This is critical where incorrect information has legal or financial impact. Example: Answering compliance or audit-related queries.

*Amazon Bedrock Knowledge Bases*

Amazon Bedrock Knowledge Bases is a managed AWS service that implements RAG. It automatically ingests data, creates embeddings, retrieves relevant context, and passes it to foundation models. This removes the need to build custom retrieval pipelines.
Business benefit: Faster deployment, lower operational complexity, and better grounded responses.
Exam cues: Keywords- “RAG,” “grounded responses,” “no fine-tuning required.”

**Identify AWS services that help store embeddings within vector databases (for example, Amazon  OpenSearch Service, Amazon Aurora, Amazon Neptune, Amazon RDS for PostgreSQL).**

Vector databases store embeddings so GenAI systems can perform similarity search and retrieval, which is essential for use cases like RAG. In the AWS AI Practitioner exam, the focus is on which AWS services can store and query vectors and why one service is chosen over another, not on low-level indexing algorithms. If the question emphasizes fast semantic or similarity search at scale, choose Amazon OpenSearch Service. If it emphasizes existing relational or graph data, choose Aurora, RDS for PostgreSQL, or Neptune accordingly.

*Amazon OpenSearch Service*

Amazon OpenSearch Service supports vector search natively, making it a common choice for GenAI and semantic search workloads. It can store embeddings and perform similarity searches using approximate nearest neighbor (ANN) techniques. OpenSearch is optimized for fast retrieval at scale and integrates well with search and analytics use cases. It is often selected when low-latency search across large datasets is required.
Business use case: Semantic document search for RAG-based applications.
Exam cues: Keywords- “vector search,” “semantic search,” “low-latency retrieval.”

*Amazon Aurora*

Amazon Aurora can store embeddings as vectors using extensions or custom schemas. It is suitable when vector search must be combined with strong transactional consistency and relational data. However, it is not optimized purely for large-scale vector similarity search compared to specialized services. Aurora is chosen when embeddings are part of a broader relational workload.
Business use case: Applications needing both transactional data and embedding storage.
Exam cues: Look for “relational database,” “transactional workloads.”

*Amazon Neptune*

Amazon Neptune is a graph database that can store embeddings alongside graph relationships. It is useful when similarity search needs to be combined with relationship-based queries. Neptune is not primarily a vector database, but embeddings can enhance graph-based reasoning and recommendations.
Business use case: Knowledge graphs enhanced with semantic similarity.
Exam cues: Keywords- “graph relationships,” “connected data.”

*Amazon RDS for PostgreSQL*

Amazon RDS for PostgreSQL supports vector storage through extensions such as pgvector. This makes it a practical choice for teams already using PostgreSQL who want to add basic vector search capabilities. While cost-effective and familiar, it may not scale as efficiently as dedicated vector search services.
Business use case: Small to medium-scale GenAI or RAG prototypes.
Exam cues: Phrases like “pgvector,” “existing PostgreSQL workloads.”

**Explain the cost tradeoﬀs of various approaches to FM customization (for example, pre-training,  ﬁne-tuning, in-context learning, RAG).**

FM customization improves relevance and accuracy, but each approach has different cost, time, and operational impacts. In the AWS AI Practitioner exam, the key is understanding that more control and specialization usually means higher cost and complexity, while lighter approaches trade precision for affordability and speed. In exam scenarios, start with in-context learning or RAG. Choose fine-tuning only if accuracy requirements cannot be met, and avoid pre-training unless explicitly justified. Highest cost- Pre-training; High cost- Fine-tuning; Low cost- In-context learning; Best cost–accuracy balance- RAG

*Pre-training*

Pre-training involves training a model from scratch on massive datasets, requiring significant compute, storage, and expert resources. This is the most expensive customization approach and is typically only feasible for large organizations or model providers. Costs include long training times, specialized hardware, and ongoing maintenance. For most businesses, pre-training is unnecessary and inefficient.
Cost impact: Very high upfront and ongoing cost.
Exam cue: “Training from scratch” = highest cost, rarely recommended.

*Fine-tuning*

Fine-tuning adapts an existing foundation model using domain-specific labeled data. It is cheaper than pre-training but still requires compute resources, data preparation, and governance. Fine-tuned models improve consistency and domain alignment but increase lifecycle management costs. Fine-tuning is justified only when prompts or RAG cannot meet accuracy needs.
Cost impact: High, but lower than pre-training.
Exam cue: Look for “domain-specific accuracy” with higher cost.

*In-context learning (prompt engineering)*

In-context learning customizes behavior by providing instructions or examples directly in the prompt. There is no training cost, making it fast and inexpensive to implement. However, longer prompts increase token usage and recurring inference costs. It also has limits in consistency and scalability.
Cost impact: Low upfront cost, variable inference cost.
Exam cue: “Prompt-based customization” = low setup cost.

*Retrieval Augmented Generation (RAG)*

RAG retrieves external data at inference time instead of changing the model itself. It avoids retraining costs and keeps data up to date, but introduces costs for storage, embedding generation, and retrieval infrastructure. Overall, it offers a strong balance between cost and accuracy for enterprise use.
Cost impact: Moderate, predictable, and scalable.
Exam cue: “Use enterprise data without retraining” = RAG.

**Describe the role of agents in multi-step tasks (for example, Amazon Bedrock Agents, agentic AI,  model context protocol).**

Agents extend GenAI beyond single prompt–response interactions by enabling models to plan, decide, and act across multiple steps. In the AWS AI Practitioner exam, agents are evaluated as orchestration mechanisms that coordinate reasoning, tool use, and context management to complete complex tasks reliably. If the question involves multi-step reasoning, tool use, or workflow execution, the correct answer usually involves agents or agentic AI, not standalone foundation models.

*What agents are and why they are needed*

Standard GenAI models respond to one prompt at a time and have no built-in memory of task progress. Agents add structure by breaking a complex goal into smaller steps, tracking state, and deciding what to do next. This makes GenAI usable for workflows rather than isolated responses.
Business example: Automating a multi-step request like searching data, generating a report, and validating results.
Exam cues: Keywords- “planning,” “task orchestration,” “multi-step workflows.”

*Agentic AI*

Agentic AI refers to systems where the model can reason about goals, select actions, and iterate until completion. The model evaluates intermediate outputs and adjusts its behavior dynamically. This increases autonomy but also requires guardrails to limit actions and ensure responsible use.
Business example: An AI assistant that decides when to retrieve data, when to generate text, and when to ask for clarification.
Exam cues: Phrases like “autonomous decision-making,” “iterative reasoning.”

*Amazon Bedrock Agents*

Amazon Bedrock Agents provide a managed way to build agents using foundation models. They handle task decomposition, API calls, and interaction with knowledge bases. This reduces custom orchestration code while keeping control through defined actions and policies.
Business example: Building an enterprise assistant that retrieves information and performs structured actions.
Exam cues: Look for “managed agents,” “tool invocation,” “Bedrock integration.”

*Model Context Protocol (MCP)*

Model Context Protocol defines how context, tools, and memory are passed to models during multi-step interactions. It ensures the model receives consistent, structured context at each step. This improves reliability and reduces errors in complex workflows.
Business example: Maintaining consistent context across multiple turns in an agent-driven process.
Exam cues: Keywords- “context management,” “structured inputs.”

*Why agents matter for business*

Agents enable GenAI to perform end-to-end tasks instead of single responses. This increases automation, reduces manual intervention, and improves consistency. However, agents must be constrained to prevent unintended actions or errors. AWS-managed agents help balance autonomy with control.


